# Lecture 8-近似梯度法

-  函数分解

次梯度法虽然说很generic，但是收敛性很慢，所以有人提出了近似梯度法，主要思路是把目标函数进行分解，分解成一个可微的$g(x)$和另外一个不必是可微的$h(x)$，即$f(x)=g(x)+h(x)$，其中$dom(g)=\mathbb{R}^n$。说明一下，这里我们考虑的都是凸函数，即$f(x),g(x),h(x)$都是凸的。

我们先考虑一个很简单的问题，即函数$f(x)$可微。好，可微课太好了，我们直接用梯度下降$x^{+}=x-t \cdot \nabla f(x)$，如果你要是嫌弃梯度下降下降得还是不够快，牛顿方法可以来加速，即

$x^{+}=\underset{z}{\arg \min } \underbrace{f(x)+\nabla f(x)^{T}(z-x)+\frac{1}{2 t}\|z-x\|_{2}^{2}}_{\tilde{f}_{t}(z)}$，这就是$f(x)$在$x$附近的二阶泰勒展开了，这玩意儿是可以直接用解析解求出来的。

可现在问题没这么理想，我们的$f(x)$是不可微的，咋办呢？我们刚才说可以把$f(x)$分解成一个可微的$g(x)$和另外一个不必是可微的$h(x)$，即$f(x)=g(x)+h(x)$，那我们干脆就二阶近似$g(x)$这部分，而$h(x)$不管它，是多少就是多少，迭代的时候我们原封不动地加上$h(x)$的取值就完事儿了。即$x^{+}=\underset{z}{\arg \min } \tilde{g}_{t}(z)+h(z)$

我们稍微做一点运算，如下：

$\begin{aligned} x^{+} &=\underset{z}{\arg \min } \tilde{g}_{t}(z)+h(z) \\ &=\underset{z}{\arg \min } g(x)+\nabla g(x)^{T}(z-x)+\frac{1}{2 t}\|z-x\|_{2}^{2}+h(z) \\ &=\underset{z}{\arg \min } \frac{1}{2 t}\|z-(x-t \nabla g(x))\|_{2}^{2}+h(z)   ,\text{   公式(1)} \end{aligned}$

倒数第二个式子和倒数第一个式子是等价的，不信的话，求出它们的一阶导数为0的点，真的是一样的。BTW，公式中的$1/t$就是$\nabla^2g(x)$。

仔细看看公式(1)，我们会发现第1项有点类似梯度下降法，但是也不完全一样，前面还有个关于$z$的项；第2项就取当前迭代最优处的$h(z)$即可。

现在，我们定义一个名叫proximal mapping的东西，如下，公式中的$还是Hessian矩阵：

$\operatorname{prox}_{t}(x)=\underset{z}{\arg \min } \frac{1}{2 t}\|z-x\|_{2}^{2}+h(z)$

于是，我们的近似梯度算法就可以描述为：

